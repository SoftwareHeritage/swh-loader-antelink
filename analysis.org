#+title: Analysis about Antepedia contents

All those numbers are actually reproducible through the antelink db.

NB: Those numbers do not account for the corrupted data on sesi yet.
The computation of checksums for those contents (including corruption flag - mismatch
between sha1 name and actual sha1) is in progress on sesi.

* Nomenclatura

- content: swh's content table (duplicated from ours in softwareheritage db)
- content_sesi: a scan of sesi's antelink backup (exactly 314899904
- contents which is more than reality now ~> 280M)
- content_s3: `aws s3 ls` from antepedia's s3 bucket (exact listing of antepedia's s3 contents)
- content_s3_not_in_sesi: {s3} \ {sesi}
- content_s3_not_in_sesi_nor_swh: ({s3} \ {sesi}) \ {swh}

* Antelink content in s3 and not in sesi

{s3} \ {sesi}:
#+begin_src sql
antelink=# create materialized view content_s3_not_in_sesi
as select sha1, path
    from content_s3 as s3
    where not exists
      (select 1 from content_sesi as sesi where s3.sha1 = sesi.sha1);

SELECT 741797
#+end_src

Thus in compressed size (unit is bytes):
#+begin_src sql
antelink=# select sum(s3.length)
from content_s3_not_in_sesi s3notsesi
inner join content_s3 s3 on s3.sha1=s3notsesi.sha1;
    sum
------------
 3543013165
(1 row)
#+end_src

so ~3G.

* Antelink contents only in s3 and not in swh nor sesi

({s3} \ {sesi}) \ {swh}:
#+begin_src sql
antelink=# create materialized view content_s3_not_in_sesi_nor_in_swh
           as select sha1, path
           from content_s3_not_in_sesi as s3
           where not exists
           (select 1 from content as swh where s3.sha1 = swh.sha1);
SELECT 46
#+end_src

So apparently, the data that needs retrieval is already on sesi.

* Antelink contents in sesi but not in swh


{sesi} \ {swh}:
#+begin_src sql
antelink=# create materialized view content_sesi_not_in_swh
           as select sha1
           from content_sesi as sesi
           where not exists
           (select 1 from content as swh where sesi.sha1 = swh.sha1);
SELECT 207095510
#+end_src

Indeed!

* Missed backup-ed contents since disks crash

Table `content_sesi` contains contents from zack's scan and
compression routine.

Since then, inria admins mistakenly removed our hard disk bay (/antelink/store0 mount point).
So we miss 25 419 313 contents.

** Demonstration

Each of the following files contains on each line one filepath in the
corresponding store (/antelink/store{10,11,12,13,14,15,16,4,5,6,7,8,9}):
#+begin_src txt
   32269834 /antelink/store0/tmp-compute-checksums/file-store10.csv
    6353200 /antelink/store0/tmp-compute-checksums/file-store11.csv
    6356584 /antelink/store0/tmp-compute-checksums/file-store12.csv
    6192914 /antelink/store0/tmp-compute-checksums/file-store13.csv
   32262055 /antelink/store0/tmp-compute-checksums/file-store14.csv
   32268595 /antelink/store0/tmp-compute-checksums/file-store15.csv
    6352648 /antelink/store0/tmp-compute-checksums/file-store16.csv
   32236548 /antelink/store0/tmp-compute-checksums/file-store4.csv
   32246790 /antelink/store0/tmp-compute-checksums/file-store5.csv
   32230119 /antelink/store0/tmp-compute-checksums/file-store6.csv
   32264976 /antelink/store0/tmp-compute-checksums/file-store7.csv
   32249986 /antelink/store0/tmp-compute-checksums/file-store8.csv
    6196342 /antelink/store0/tmp-compute-checksums/file-store9.csv
  289480591 total
#+end_src

#+begin_src elisp
(- 314899904 289480591) ;; 25 419 313
#+end_src

* Hash computation running on sesi:

|---------+-----------+-----------+----------+-----------+-----------|
| Store   |  Expected |    Actual |  Remains |      Done | injected? |
|---------+-----------+-----------+----------+-----------+-----------|
| store10 |  32269834 |  32269834 |        0 |       100 | X         |
| store11 |   6353200 |   6353200 |        0 |       100 | X         |
| store12 |   6356584 |   6356584 |        0 |       100 | X         |
| store13 |   6192914 |   6192914 |        0 |       100 | X         |
| store14 |  32262055 |  32262055 |        0 |       100 | X         |
| store15 |  32268595 |  32268595 |        0 |       100 | X         |
| store16 |   6352648 |   6352648 |        0 |       100 | X         |
| store4  |  32236548 |  27374993 |  4861555 | 84.919120 |           |
| store5  |  32246790 |  22091436 | 10155354 | 68.507396 |           |
| store6  |  32230119 |  18100039 | 14130080 | 56.158772 |           |
| store7  |  32264976 |  20761797 | 11503179 | 64.347784 |           |
| store8  |  32249986 |  20282705 | 11967281 | 62.892136 |           |
| store9  |   6196342 |   6196342 |        0 |       100 | X         |
|---------+-----------+-----------+----------+-----------+-----------|
| total   | 289480591 | 236863142 | 52617449 | 81.823497 | 128251802 |
|---------+-----------+-----------+----------+-----------+-----------|
#+TBLFM: $4=$2-$3::@15$2=vsum(@2$2..@14$2)::@15$3=vsum(@2$3..@14$3)::$5=(100*$3)/$2

Note: There remains:
- huge file to deal with (~370)

#+begin_src sql
antelink=> select count(*) from content_sesi_all;
   count
-----------
 128251802
(1 row)
#+end_src

# (+ 32269834  6353200  6356584  6192914 32262055 32268595  6352648 6196342) ;; 128252172

# delta: (- 128252172 128251802);; 370  (this matched the skipped entries - >2g compressed files - in csv inputs)

Skipped files:
#+begin_src sh
ardumont@worker08:~$ grep skipped stats/inject-checksums-* | wc -l
370
#+end_src

** Estimated average speed

#+begin_src elisp
(require 'dash)
(require 'parse-time)

(defun swh-worker-to-time (str-date)
  "STR-DATE is a parseable string date.
Compute the time."
  (->> str-date
       parse-time-string
       (apply 'encode-time)))

(defun swh-worker-time-in-seconds-between (str-date1 str-date0)
  "Compute the seconds between STR-DATE1 and STR-DATE0.
They are both string parseable date time."
  (let ((t1 (swh-worker-to-time str-date1))
        (t0 (swh-worker-to-time str-date0)))
    (-> (time-subtract t1 t0)
        time-to-seconds
        abs)))

(defun swh-worker-average-speed-per-day (str-date1 num-date1 str-date0 num-date0)
  "Compute the average speed per day.
STR-DATE1 snapshot time t1
NUM-DATE1 number of computations done at t1
STR-DATE0 snapshot time t0
NUM-DATE0 number of computations done at t0."
  (let ((seconds (swh-worker-time-in-seconds-between str-date0 str-date1))
        (hashes-diff (abs (- num-date0 num-date1))))
    (/ hashes-diff seconds)))
#+end_src

So ~465.5 hashes/s
#+begin_src elisp
(swh-worker-average-speed-per-day "Mon Feb 22 11:29:40 CET 2016" 238404724 "Mon Feb 22 11:16:46 CET 2016" 238044401);; 465.53359173126614
(swh-worker-average-speed-per-day "Mon Feb 22 14:48:41 CET 2016" 245227449 "Mon Feb 22 11:16:46 CET 2016" 238044401);; 564.927093983484
#+end_src

** Remains

So at current speed, remains ~1.1 days left
#+begin_src elisp
(defun swh-worker-remains-in-days (speed-per-second remain-comps)
  "Compute the remaining days given SPEED-PER-SECOND and REMAIN-COMPS to do."
  (let ((remaining-time (/ remain-comps speed-per-second))
        (nb-seconds-in-a-day (* 60.0 60.0 24.0)))
    (/ remaining-time nb-seconds-in-a-day)))

(let* ((snap-date0 "Mon Feb 22 11:16:46 CET 2016")
       (comps-done-date0 238044401)
       (snap-date1 "Mon Feb 22 14:48:41 CET 2016")
       (comps-done-date1 245227449)
       (total-comps-to-do 289480591)
       (remain-comp (- total-comps-to-do comps-done-date1)))
  (-> (swh-worker-average-speed-per-day snap-date0 comps-done-date0 snap-date1 comps-done-date1)
      (swh-worker-remains-in-days remain-comp)))
;; 0.9066464486716451
#+end_src
